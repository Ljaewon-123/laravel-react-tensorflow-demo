# ê°ì • ë¶„ì„ ëª¨ë¸ - IMDB ì˜í™” ë¦¬ë·°
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬: pip install tensorflow numpy

import tensorflow as tf
from tensorflow import keras
import numpy as np
import json

print("TensorFlow ë²„ì „:", tf.__version__)

# ==================== ì„¤ì • ====================
VOCAB_SIZE = 10000      # ì‚¬ìš©í•  ë‹¨ì–´ ê°œìˆ˜
MAX_LENGTH = 200        # ë¦¬ë·° ìµœëŒ€ ê¸¸ì´
EMBEDDING_DIM = 16      # ë‹¨ì–´ ë²¡í„° ì°¨ì›
EPOCHS = 10             # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
BATCH_SIZE = 512        # ë°°ì¹˜ í¬ê¸°

# ==================== 1. ë°ì´í„° ë¡œë“œ ====================
print("\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë”© ì¤‘...")
(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(
    num_words=VOCAB_SIZE
)

print(f"í›ˆë ¨ ë°ì´í„°: {len(x_train)}ê°œ")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(x_test)}ê°œ")
print(f"ì²« ë²ˆì§¸ ë¦¬ë·° ê¸¸ì´: {len(x_train[0])}ë‹¨ì–´")
print(f"ì²« ë²ˆì§¸ ë¦¬ë·° ë ˆì´ë¸”: {'ê¸ì •' if y_train[0] == 1 else 'ë¶€ì •'}")

# ==================== 2. ë°ì´í„° ì „ì²˜ë¦¬ ====================
print("\n[2ë‹¨ê³„] ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

# íŒ¨ë”©: ëª¨ë“  ë¦¬ë·°ë¥¼ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶¤
x_train = keras.preprocessing.sequence.pad_sequences(
    x_train, 
    maxlen=MAX_LENGTH,
    padding='post',      # ë’¤ì— 0 ì¶”ê°€
    truncating='post'    # ê¸¸ë©´ ë’¤ì—ì„œ ìë¥´ê¸°
)

x_test = keras.preprocessing.sequence.pad_sequences(
    x_test,
    maxlen=MAX_LENGTH,
    padding='post',
    truncating='post'
)

print(f"ì „ì²˜ë¦¬ í›„ shape: {x_train.shape}")

# ==================== 3. ëª¨ë¸ êµ¬ì¶• ====================
print("\n[3ë‹¨ê³„] ëª¨ë¸ êµ¬ì¶• ì¤‘...")

model = keras.Sequential([
    # Embedding: ë‹¨ì–´ ID â†’ ë²¡í„°ë¡œ ë³€í™˜
    keras.layers.Embedding(
        input_dim=VOCAB_SIZE,
        output_dim=EMBEDDING_DIM,
        input_length=MAX_LENGTH,
        name='embedding'
    ),
    
    # GlobalAveragePooling: ëª¨ë“  ë‹¨ì–´ ë²¡í„°ì˜ í‰ê· 
    keras.layers.GlobalAveragePooling1D(name='pooling'),
    
    # Dense: íŒ¨í„´ í•™ìŠµ
    keras.layers.Dense(16, activation='relu', name='hidden'),
    
    # Dropout: ê³¼ì í•© ë°©ì§€
    keras.layers.Dropout(0.5, name='dropout'),
    
    # ì¶œë ¥ì¸µ: 0(ë¶€ì •) ~ 1(ê¸ì •)
    keras.layers.Dense(1, activation='sigmoid', name='output')
], name='sentiment_model')

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
model.summary()

# ==================== 4. ëª¨ë¸ í•™ìŠµ ====================
print("\n[4ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

history = model.fit(
    x_train, y_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=0.2,  # í›ˆë ¨ ë°ì´í„°ì˜ 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ
    verbose=1
)

# ==================== 5. ëª¨ë¸ í‰ê°€ ====================
print("\n[5ë‹¨ê³„] ëª¨ë¸ í‰ê°€ ì¤‘...")

test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"\ní…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy*100:.2f}%")
print(f"í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.4f}")

# ==================== 6. ëª¨ë¸ ì €ì¥ ====================
print("\n[6ë‹¨ê³„] ëª¨ë¸ ì €ì¥ ì¤‘...")

# ëª¨ë¸ ì €ì¥
model.save('sentiment_model.h5')
print("âœ“ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: sentiment_model.h5")

# ë‹¨ì–´ ì¸ë±ìŠ¤ ì €ì¥ (ë‚˜ì¤‘ì— ì˜ˆì¸¡í•  ë•Œ í•„ìš”)
word_index = keras.datasets.imdb.get_word_index()
reverse_word_index = {value: key for key, value in word_index.items()}

# ì²˜ìŒ 100ê°œ ë‹¨ì–´ë§Œ ì €ì¥ (ì „ì²´ëŠ” ë„ˆë¬´ í¼)
word_dict = {k: v for k, v in list(word_index.items())[:100]}
with open('word_index.json', 'w', encoding='utf-8') as f:
    json.dump(word_dict, f, ensure_ascii=False, indent=2)
print("âœ“ ë‹¨ì–´ ì‚¬ì „ ì €ì¥ ì™„ë£Œ: word_index.json")

# ==================== 7. ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ====================
print("\n[7ë‹¨ê³„] ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")

def decode_review(encoded_review):
    """ìˆ«ìë¡œ ëœ ë¦¬ë·°ë¥¼ ì›ë˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])

def predict_sentiment(model, review_text, word_index):
    """ìƒˆë¡œìš´ ë¦¬ë·°ì˜ ê°ì • ì˜ˆì¸¡"""
    # í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜
    words = review_text.lower().split()
    encoded = [word_index.get(word, 0) + 3 for word in words]
    
    # íŒ¨ë”©
    padded = keras.preprocessing.sequence.pad_sequences(
        [encoded], 
        maxlen=MAX_LENGTH,
        padding='post'
    )
    
    # ì˜ˆì¸¡
    prediction = model.predict(padded, verbose=0)[0][0]
    
    return prediction

# í…ŒìŠ¤íŠ¸ ë¦¬ë·° ëª‡ ê°œ ì˜ˆì¸¡í•´ë³´ê¸°
test_indices = [0, 100, 200]
print("\n" + "="*60)
print("ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼:")
print("="*60)

for idx in test_indices:
    review = decode_review(x_test[idx])
    actual = "ê¸ì •" if y_test[idx] == 1 else "ë¶€ì •"
    
    prediction = model.predict(x_test[idx:idx+1], verbose=0)[0][0]
    predicted = "ê¸ì •" if prediction > 0.5 else "ë¶€ì •"
    
    print(f"\në¦¬ë·° #{idx}")
    print(f"ë‚´ìš©: {review[:100]}...")
    print(f"ì‹¤ì œ: {actual} | ì˜ˆì¸¡: {predicted} (í™•ë¥ : {prediction:.2%})")
    print(f"{'âœ“ ì •ë‹µ' if actual == predicted else 'âœ— ì˜¤ë‹µ'}")

# ==================== 8. í•™ìŠµ ê³¡ì„  ì‹œê°í™” ì¤€ë¹„ ====================
print("\n[8ë‹¨ê³„] í•™ìŠµ ê²°ê³¼ ìš”ì•½...")

print("\nì—í¬í¬ë³„ ì •í™•ë„:")
for epoch, (train_acc, val_acc) in enumerate(zip(
    history.history['accuracy'], 
    history.history['val_accuracy']
), 1):
    print(f"Epoch {epoch:2d}: í›ˆë ¨ {train_acc:.4f} | ê²€ì¦ {val_acc:.4f}")

print("\n" + "="*60)
print("í•™ìŠµ ì™„ë£Œ! ğŸ‰")
print("="*60)
print(f"ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy*100:.2f}%")
print(f"ì €ì¥ëœ íŒŒì¼:")
print(f"  - sentiment_model.h5 (ëª¨ë¸)")
print(f"  - word_index.json (ë‹¨ì–´ ì‚¬ì „)")
print("\në‹¤ìŒ ë‹¨ê³„: ì´ ëª¨ë¸ì„ ì›¹ ì„œë¹„ìŠ¤ë¡œ ë§Œë“¤ì–´ë³´ì„¸ìš”!")

